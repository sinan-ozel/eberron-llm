{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a16cd04-d040-47b8-bf3a-831331814156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from tempfile import mkdtemp\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import login as hf_login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52283cb-d51e-489a-9c6c-e66f32305579",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEFACT_VERSION = '01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3029e0-ed9f-476b-aa5f-9e2408f96e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install airllm==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e376d40-9f79-43c8-bcf4-4b10b71a47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEFACT_ROOT_FOLDER = os.environ.get('ARTEFACT_ROOT_FOLDER', '/artefact')\n",
    "ARTEFACT_FOLDER = os.path.join(ARTEFACT_ROOT_FOLDER, 'eberron', f'v{ARTEFACT_VERSION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc081ee2-803c-40de-af67-631bb34243b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written on ChatGPT 4.0 with the prompt: Write a python class to temporarily set an environmental variable. I need to use the class with `with`. I want the variable to be unset upon exit if it was not set at entry, or set back to its original value.\n",
    "\n",
    "class TempEnvVar:\n",
    "    def __init__(self, key, value):\n",
    "        \"\"\"\n",
    "        Initialize the TempEnvVar object with the environment variable key and temporary value.\n",
    "        \n",
    "        Args:\n",
    "            key (str): The environment variable name.\n",
    "            value (str): The temporary value to set.\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.original_value = None\n",
    "        self.was_set = False\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"\n",
    "        Set the environment variable when entering the with block.\n",
    "        \"\"\"\n",
    "        self.original_value = os.environ.get(self.key)\n",
    "        self.was_set = self.key in os.environ\n",
    "        os.environ[self.key] = self.value\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"\n",
    "        Restore or unset the environment variable when exiting the with block.\n",
    "        \"\"\"\n",
    "        if self.was_set:\n",
    "            os.environ[self.key] = self.original_value\n",
    "        else:\n",
    "            os.environ.pop(self.key, None)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e262d-763d-45c7-991f-bf5518f2fd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6441db-8d18-43d6-896f-e194905c5f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7363c9cd-e17e-4585-8d4c-2dce6623d16f",
   "metadata": {},
   "source": [
    "# Load the Artefact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c12afcf-8df7-4a38-be26-ce04d62cb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTEFACT_FOLDER, 'embeddings.pkl'), 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41f010e-c357-4954-8288-0bf301e91eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTEFACT_FOLDER, 'model_metadata.json'), 'r') as f:\n",
    "    model_metadata = json.load(f)\n",
    "assert model_metadata['embedding_model']['str'].startswith('SentenceTransformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35a46545-7b93-45ca-a198-228006934b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ARTEFACT_FOLDER, 'chunk_metadata.json'), 'r') as f:\n",
    "    chunk_metadata = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc6a55e-961f-4bd2-bace-fc6e20a8d649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b15b45294b4fbeb816904a36410614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_names = [f for f in os.listdir(os.path.join(ARTEFACT_FOLDER, 'chunks')) if f.endswith('.md')]\n",
    "file_names = sorted(file_names)\n",
    "chunks = [None] * len(file_names)\n",
    "for file_name in tqdm(file_names):\n",
    "    file_path = os.path.join(ARTEFACT_FOLDER, 'chunks', file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        chunks[int(file_name.split('.')[0])] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3f1ad-d5e3-403f-b8fd-11f4997b5e8d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae2ea462-f58d-4a79-ab6e-b3b343af7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with BM25 and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f43fb-ff45-455e-965e-bb509e6deb62",
   "metadata": {},
   "source": [
    "## BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06660842-a2fb-42e6-9918-8fefbd4821cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:         2863804 kB\n",
      "MemAvailable:   30576364 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a6fcb9-fa6e-436e-ac87-379c5f87c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 1 MiB, 15095 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d99d18-171b-4a59-aa74-61b2f3fa000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chunks = [chunk.lower().split(\" \") for chunk in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "552e3af2-a71d-4a79-b20e-2fde272e27a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:         1694128 kB\n",
      "MemAvailable:   29406908 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1239221c-c705-4155-b0fb-552e78d8fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 1 MiB, 15095 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd64783-c87e-4857-8ec1-0b3272db9f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alibaba-NLP/gte-base-en-v1.5'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metadata['embedding_model']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1147af-6135-4090-acf6-80b984a5f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 02:10:17.560398: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-10 02:10:17.575265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-10 02:10:17.594916: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-10 02:10:17.600868: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-10 02:10:17.615201: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-10 02:10:18.777113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(model_metadata['embedding_model']['name'], \n",
    "                                      trust_remote_code=True, \n",
    "                                      revision=model_metadata['embedding_model']['revision'])\n",
    "embedding_model = embedding_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c20117f2-6fd0-4333-abf5-446d7831751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          677232 kB\n",
      "MemAvailable:   28565060 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f5e28e7-ece2-4e29-9ff9-2f99ea16dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 671 MiB, 14425 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4280d4f2-d3d5-4b73-a5c2-dc93c63f2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Who is Dash Donnigan?\"\n",
    "# query = \"Who is Commander Iyanna?\"\n",
    "# query = \"Tell me about Menthis Plateau.\"\n",
    "# query = \"Tell me about Eldeen Reaches.\"\n",
    "# query = \"Tell me about the rivers of Khorvaire.\"\n",
    "\n",
    "# query = \"Tell me about Xen'drik.\"\n",
    "# query = \"Tell me about fashion in Khorvaire.\"\n",
    "# query = \"Create a House Cannith item.\"\n",
    "user_query = \"Tell me about the languages of Eberron.\"\n",
    "query_embed = embedding_model.encode(user_query, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62fa6a1e-36f5-412a-8cf5-c4a51f46b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          674892 kB\n",
      "MemAvailable:   28562720 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ac69426-2e6c-4611-a3e5-6e512ce26019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 671 MiB, 14425 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3783cc-028e-427a-8d1b-f6b225c505dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df1fc4-ac0d-4dc5-8a86-c054fe28e579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e932a3-8a48-499a-b7dc-45ad2695f2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2719, 4202, 4203]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "similarities = torch.from_numpy(np.dot(embeddings, query_embed.T))\n",
    "\n",
    "similarities.topk(k).indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "656c4290-9908-4df7-8aa7-64b7d935e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'Eberron_ Rising From the Last War - Jeremy Crawford & James Wyatt & Keith Baker.pdf', 'edition': '5e', 'pdf/title': 'Eberron: Rising From the Last War', 'pdf/author': 'Jeremy Crawford & James Wyatt & Keith Baker', 'pages': [5, 6, 7]}\n",
      "{'filename': '1598836-Languages_of_Eberron_2E.pdf', 'edition': '5e', 'pdf/title': 'Languages of Eberron 2E mk iii', 'pdf/author': '', 'pages': [2, 3, 4]}\n",
      "{'filename': '1598836-Languages_of_Eberron_2E.pdf', 'edition': '5e', 'pdf/title': 'Languages of Eberron 2E mk iii', 'pdf/author': '', 'pages': [3, 4, 5]}\n"
     ]
    }
   ],
   "source": [
    "for i in similarities.topk(k).indices.tolist():\n",
    "    print(chunk_metadata[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28893004-49e7-4cfd-898b-f383f5655c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': '1598836-Languages_of_Eberron_2E.pdf',\n",
       "  'edition': '5e',\n",
       "  'pdf/title': 'Languages of Eberron 2E mk iii',\n",
       "  'pdf/author': '',\n",
       "  'pages': [1, 2, 3]},\n",
       " {'filename': '1598836-Languages_of_Eberron_2E.pdf',\n",
       "  'edition': '5e',\n",
       "  'pdf/title': 'Languages of Eberron 2E mk iii',\n",
       "  'pdf/author': '',\n",
       "  'pages': [2, 3, 4]},\n",
       " {'filename': '1598836-Languages_of_Eberron_2E.pdf',\n",
       "  'edition': '5e',\n",
       "  'pdf/title': 'Languages of Eberron 2E mk iii',\n",
       "  'pdf/author': '',\n",
       "  'pages': [3, 4, 5]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(user_query.lower().split(\" \"), chunk_metadata, n=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09da4db-2073-4632-b93f-5768c37763f6",
   "metadata": {},
   "source": [
    "# Answer Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9f60bbf-a14a-4cd5-a9b3-797c01823cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d71619c-f703-4619-ae06-989230bd056c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 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\n",
    "# hf_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "648d0f35-c694-4b39-ab1e-525aefd6c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4aa6090-f488-45f8-92b6-ba666ed51673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          675552 kB\n",
      "MemAvailable:   28563408 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31523b26-7383-413d-9724-a4aa2a561a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 671 MiB, 14425 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "274e4210-6262-40de-a557-62e81237a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from airllm import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1445f0d6-76e9-490e-a7fb-c1962b39998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f95088-8aad-4a6e-8a12-da9702f3e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/jupyterlab/models/hf/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d5179-8b5c-498c-a7f8-55df4206193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "109839ef-5169-4304-b67e-97bc86d1b3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387c3cbcd6f540e49fb58b3e42aefd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/jupyterlab/models/hf/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, quantization_config=bnb_config, device_map=\"balanced_low_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "168a3189-ae2a-432a-96ed-aa6cded672bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          361912 kB\n",
      "MemAvailable:   27970812 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd0165a5-2369-4576-ab47-38610d971acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 4087 MiB, 11009 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b4ec852-3a54-476d-aaf6-13c354e0e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with TempEnvVar('TRANSFORMERS_OFFLINE', '1'):\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "409ad644-a61d-4ee1-bb07-692d7aa697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b98d1cb-35b2-4a05-a588-1a3ad5357294",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = \"\\n\\n\\n\".join([chunks[i] for i in similarities.topk(1).indices.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d07c14d4-09fc-4bd0-b7ec-4f2fc7f3e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with TempEnvVar('TRANSFORMERS_OFFLINE', '1'):\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "481c0f10-dd91-476d-b863-e10cb1ae865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"{system_prompt}\n",
    "\n",
    "Here is the retrieved context:\n",
    "{retrieved_docs}\n",
    "\n",
    "Here is the users query:\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "formatted_prompt = f\"Q: {prompt} A: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d48d033e-ed6a-4e57-b3f3-28cc438b414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tokens = model.tokenizer([prompt],\n",
    "#     return_tensors=\"pt\", \n",
    "#     return_attention_mask=False, \n",
    "#     truncation=True, \n",
    "#     max_length=128, \n",
    "#     padding=False)\n",
    "# input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04011800-f871-4e00-84be-7e12f39d66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_output = model.generate(\n",
    "#     input_tokens['input_ids'].cuda(), \n",
    "#     max_new_tokens=20,\n",
    "#     use_cache=True,\n",
    "#     return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797091f-b628-4683-8f82-d8e76208c480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f053ae87c80b44e6be5c36bee6df8feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, quantization_config=bnb_config, device_map=\"auto\")\n",
    "# with TempEnvVar('TRANSFORMERS_OFFLINE', '1'):\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, quantization_config=bnb_config, device_map=\"balanced_low_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53ddb773-ce76-442d-b2cb-4901e0c90753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c930e600-ccf8-495c-aa7e-c5f1d59f389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          360924 kB\n",
      "MemAvailable:   27969896 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8292bb61-7d2c-4479-a339-bb6a669a4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 4087 MiB, 11009 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6558c1b2-c722-4237-9e94-bfe51801ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with TempEnvVar('TRANSFORMERS_OFFLINE', '1'):\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49beb1e5-484f-4c3a-b995-0a1a28504324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(param.device.type == 'cuda' for param in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99299481-6646-46fe-8e08-ec6c9b9c5bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(param.device.type == 'cpu' for param in embedding_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "595a740c-2d6f-4926-ba7e-e04c5e5c5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          359092 kB\n",
      "MemAvailable:   27968448 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e1674c3-e466-4abc-be56-1ce9f9bce0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 4087 MiB, 11009 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b5fc9b5-a0b2-4b71-8284-3a22440e1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c12123b3-bfe2-4b7c-8f98-e46cf15040ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       32386472 kB\n",
      "MemFree:          349772 kB\n",
      "MemAvailable:   27959312 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep Mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f24246a-6c3a-43fc-ab17-94b475c75c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 4087 MiB, 11009 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "86128ef6-0038-4231-93aa-7193d812bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "# model_inputs = tokenizer([user_query], return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ee0e14f2-4cca-4303-adc5-ea88ec671298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=4667, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "531823d1-8cdd-4f4b-8af4-b2f09da75ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16604, 2, 4667)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_prompt), len(model_inputs), len(model_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "76e70fb3-4234-41ab-8608-ccaf7a12d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 5957 MiB, 9139 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "400c51e8-3c11-4e11-9d7d-95d03c3d56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = model.generate(**model_inputs, max_new_tokens=50, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c511924f-a0dc-4e49-8622-d3a4c39651ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 5957 MiB, 9139 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92c044aa-70fc-4962-bd98-e8eb64ec3ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81cd7d54-3df4-4937-aac9-f1819b13ede0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8b558def-82a1-45ed-ad74-53c0371afa4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5179"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db1c12f0-7282-4431-b9ad-47b60239e71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In Eberron, languages reflect culture and geography; a dwarf raised in Breland might not know Dwarvish, but a halfling raised in the Mror Holds might. The historical development of languages and cultures also explains the scripts used to write various languages. For example, the Orc language is written using the Goblin script (rather than Dwarvish, as stated in the Player's Handbook), because the orcs of Khorvaire learned writing from the goblins. Common is the language of the Five Nations and the language of trade in Khorvaire, known by most of its people. Goblin was the trade language of the goblin empire of Dhakaan and survives as the primary language in Darguun, Droaam, and the Shadow Marches regions. Goblin displaced the Orc language; the people of the Shadow Marches typically speak Goblin, and Orc is an exotic language. Members of all races in Xen'drik speak Giant and use it as their trade language. Abyssal is the common tongue of all fiends. Abyssal is sometimes called “Khyber’s Speech,” while Celestial is “the tongue of Siberys.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True, stream=False)\n",
    "output_text.split('A:  ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1370e57b-e095-4f9e-8743-7dc5bb0c48f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "15360 MiB, 5957 MiB, 9139 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8c3846b3-142a-4cd4-8d15-578d8c50c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutOfMemoryError: CUDA out of memory. Tried to allocate 56.35 GiB. GPU 0 has a total capacity of 14.74 GiB of which 5.05 GiB is free. Process 14461 has 9.68 GiB memory in use. Of the allocated memory 8.48 GiB is allocated by PyTorch, and 1.08 GiB is reserved by PyTorch but unallocated. \n",
    "# If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f10adc9-776a-4501-b9f2-5c568670aadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PID TTY          TIME CMD\n",
      "    210 pts/0    00:00:00 ps\n"
     ]
    }
   ],
   "source": [
    "!ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d11345-7613-4684-9dc2-266c4336c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutOfMemoryError: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 310.12 MiB is free. Process 31577 has 14.44 GiB memory in use. Of the allocated memory 13.09 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. \n",
    "# If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fdc2f-c6eb-4010-978a-ddf8691998b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4afb7-583e-43b5-a560-a9861d6edf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model(formatted_prompt, max_tokens=800, stop=[\"Q:\", \"\\n\"], echo=True, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1ca56-ee94-4219-b614-0782c00475d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb8bc3-1911-41e2-8a8b-0f475e2020bb",
   "metadata": {},
   "source": [
    "# Create Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc48371-ccb2-4e8f-8bd8-1de7e72533c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dev/nvme2n1     25G   15G   11G  59% /jupyterlab\n"
     ]
    }
   ],
   "source": [
    "!df -ah | grep jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504255d-a871-42c2-9af0-12d5ae58f8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
